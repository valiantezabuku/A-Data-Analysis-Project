{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indian Start-up Funding Analysis\n",
    "\n",
    "\n",
    "### Project Scenario\n",
    "The Team is trying to venture into the Indian start-up ecosystem. As the data expert of the team, I was tasked to:\n",
    "\n",
    "- Investigate the ecosystem and propose the best course of action.\n",
    "  - Analyze funding received by start-ups in India from 2018 to 2021.\n",
    "  - Separate data for each year of funding will be provided.\n",
    "  - Find the start-upsâ€™ details, the funding amounts received, and the investors' information.\n",
    "\n",
    "### Business Understanding\n",
    "The Indian startup ecosystem has experienced significant growth and investment over the past few years. As a data expert tasked with investigating this ecosystem, our goal is to analyze the funding received by startups in India from 2018 to 2021 and provide insights to guide investment decisions. By examining the details of startups, funding amounts, and investor information, we aim to understand the trends, opportunities, and challenges within the Indian startup landscape.\n",
    "\n",
    "### Objective\n",
    "To analyze funding trends and dynamics within the Indian start-up ecosystem from 2018 to 2021 and propose strategic recommendations for the team's venture.\n",
    "\n",
    "### Hypothesis Testing\n",
    "Null Hypothesis(Ho): There is no siginificate difference in the amount of funding between startups in Bangalore.\n",
    "Alternative Hypothesis(Ha): There is a siginificate difference in the amount of funding between startups in Banaglore.\n",
    "\n",
    "### Business Questions\n",
    "####\n",
    "1.What sectors have shown the highest growth in terms of funding received over the past four years?\n",
    "\n",
    "2.What locations within India have emerged as the primary hubs for startup activity and investment, and what factors contribute to their prominence?\n",
    "\n",
    "3.Are there any notable differences in funding patterns between early-stage startups and more established companies?\n",
    "\n",
    "4.Which sectors recieve the lowest level of funding and which sectors recieve the highest levels of funding in India.\n",
    "\n",
    "5.Which investors have more impact on startups over the years?\n",
    "\n",
    "6.What are the key characteristics of startups that successfully secure funding, and how do they differ from those that struggle to attract investment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values \n",
    "import pyodbc \n",
    "import numpy as np\n",
    "import pandas as pd                      \n",
    "import re     \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics as stat \n",
    "import geopandas as gpd\n",
    "#import geoplot as gplt\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "from scipy import stats        \n",
    "#from geopy.geocoders import Nominatim    \n",
    "from scipy.stats import ttest_ind    \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ðŸ›¬ Imported all packages.\", \"Warnings hidden. ðŸ‘»\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Data Set\n",
    "The first data was from a database management system, that is MIRCORSOFT SQL SERVER. Connection was made to the database using an Open Database Connectivity standard library, pyodbc. <br>\n",
    "Two tables were read from the databases. That is, <br>\n",
    "Table 1: dbo.LP1_startup_funding2020 <br>\n",
    "Table 2: dbo.LP1_startup_funding2021\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file into a dictionary\n",
    "environment_variables = dotenv_values('.env')\n",
    "\n",
    "# Get the values for the credentials you set in the '.env' file\n",
    "server = environment_variables.get(\"SERVER\")\n",
    "database = environment_variables.get(\"DATABASE\")\n",
    "username = environment_variables.get(\"USERNAME\")\n",
    "password = environment_variables.get(\"PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a connection string\n",
    "connection_string = f\"Driver={{SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the connect method of the pyodbc library and pass in the connection string.\n",
    "# This will connect to the server and might take a few seconds to be complete. \n",
    "# Check your internet connection if it takes more time than necessary\n",
    "connection = pyodbc.connect(connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the sql query to get the data is what what you see below.\n",
    "# Note that you will not have permissions to insert delete or update this database table.\n",
    "query = \"Select * from dbo.LP1_startup_funding2020\"\n",
    "table_1 = pd.read_sql(query, connection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Select * from dbo.LP1_startup_funding2021\"\n",
    "table_2 = pd.read_sql(query, connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_1[table_1['column10'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop column10 in table_1 since it has just two values that are not null and are also just repetition of values in Stage column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop column10\n",
    "table_1.drop('column10', axis=1, inplace=True) if 'column10' in table_1.columns else table_1\n",
    "table_1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the column names\n",
    "\n",
    "def stripper(string: str, strip: list) -> str:\n",
    "    \"\"\"Strips a strip list from a given string and returns the string\"\"\"\n",
    "    for s in strip:\n",
    "        string = string.replace(s, '')\n",
    "        \n",
    "    return string\n",
    "    \n",
    "def replacer(string: str, replace: list) -> str:\n",
    "    \"\"\"Replaces each character in replace list with underscore given a string and returns the string\"\"\"\n",
    "    for r in replace:\n",
    "        string = string.replace(r, '_')\n",
    "                \n",
    "    return string\n",
    "    \n",
    "def clean_column_names(df):\n",
    "    strip   = ['(', ')', '$']\n",
    "    replace = [' ', '/'] \n",
    "    df.columns = [replacer(stripper(col_name.lower(), strip), replace) for col_name in df.columns]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the column names\n",
    "table_1 = clean_column_names(table_1)\n",
    "table_2 = clean_column_names(table_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create year column to identify each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 1: dbo.LP1_startup_funding2020\n",
    "table_1['year'] = 2020\n",
    "\n",
    "# Table 2: dbo.LP1_startup_funding2021\n",
    "table_2['year'] = 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Data Set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Data set\n",
    "first_dataset = pd.concat([table_1, table_2], ignore_index=True)\n",
    "\n",
    "first_dataset.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_dataset['amount'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "second_dataset = pd.read_csv('datasets/individual_csv/startup_funding2019.csv')\n",
    "\n",
    "second_dataset = clean_column_names(second_dataset)\n",
    "\n",
    "second_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_dataset['amount'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "third_dataset = pd.read_csv('datasets/individual_csv/startup_funding2018.csv')\n",
    "\n",
    "third_dataset = clean_column_names(third_dataset)\n",
    "\n",
    "third_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_dataset['amount'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix location values, take every letter before the first comma as headquarter\n",
    "third_dataset['location'] = [location.split(',')[0] for location in third_dataset['location']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename round_series to stage and location to headquarter\n",
    "third_dataset.rename(columns={\n",
    "    'company_name': 'company_brand', \n",
    "    'industry': 'sector', \n",
    "    'round_series': 'stage', \n",
    "    'about_company': 'what_it_does', \n",
    "    'location': 'headquarter'\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "third_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Year column to identify each dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create year column\n",
    "\n",
    "# The second data is a flat-file name startup_funding2019.csv\n",
    "second_dataset['year'] = 2019\n",
    "\n",
    "# The third part of the data flat-file named startup_funding2018.csv\n",
    "third_dataset['year']  = 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the final concatenated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Data set\n",
    "final_dataset = pd.concat([first_dataset, second_dataset, third_dataset], ignore_index=True)\n",
    "\n",
    "final_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Headquarter & Sector column\n",
    "    - If column value contains '#REF!', clean it and shift the row starting from that column by 1 step and until the stage column\n",
    "    - Sanitize sector column if after cleaning and shifting, but the sector value is also present among unique values of the headquarter column\n",
    "    - Fixes index 1297, 1312, 2155"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove '#REF!' in a series\n",
    "def remove_ref(value):\n",
    "    if isinstance(value, str):\n",
    "        value = value.replace('#REF!', '').strip()\n",
    "            \n",
    "    return value\n",
    "\n",
    "# Columns of Interest \n",
    "columns = ['headquarter', 'investor']\n",
    "for column in columns:    \n",
    "    # Identify rows where column value contains '#REF!\n",
    "    mask = final_dataset[column].str.contains('#REF!')\n",
    "    \n",
    "    # Fill missing values in mask with False\n",
    "    mask.fillna(False, inplace=True)\n",
    "    \n",
    "    # Update the column by applying the remove_ref function to the column\n",
    "    final_dataset.loc[mask, column] = final_dataset.loc[mask, column].apply(remove_ref)\n",
    "    \n",
    "    # Shift values in selected rows excluding the last column 'year'\n",
    "    final_dataset.loc[mask, column:'stage'] = final_dataset.loc[mask, column:'stage'].shift(1, axis=1)\n",
    "\n",
    "\n",
    "# Sanitisizing the sector column after shifting\n",
    "mask = final_dataset['sector'].apply(lambda x: x in final_dataset['headquarter'].unique())\n",
    "\n",
    "# Update 'headquarter' value with 'sector' value\n",
    "final_dataset.loc[mask, 'headquarter'] = final_dataset.loc[mask, 'sector']\n",
    "\n",
    "# Set the 'sector' value to NaN\n",
    "final_dataset.loc[mask, 'sector'] = np.nan          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace 'None' string values with NaN element-wise allowing for consistent representation of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function replace None with NaN\n",
    "def replace_none(value):\n",
    "    if isinstance(value, str) and (value.lower() == 'none' or value.lower() == 'nan'):\n",
    "        value = np.nan\n",
    "    \n",
    "    return value\n",
    "\n",
    "# Apply the function to all columns\n",
    "final_dataset = final_dataset.applymap(replace_none) # element-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If a value in stage column is a website link, its correct value is NaN\n",
    "\n",
    "**Website link in stage column is https://docs.google.com/spreadsheets/d/1x9ziNeaz6auNChIHnMI8U6kS7knTr3byy_YBGfQaoUA/edit#gid=1861303593**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove website link from stage column\n",
    "def remove_website_link(value):\n",
    "    # Regular expression pattern to match website URLs that begin with http:// or https:// with an optional www\n",
    "    pattern = r'https?://(?:www\\.)?\\w+\\.\\w+(?:/\\S*)?'\n",
    "    \n",
    "    # Check if the value is a string and matches the pattern\n",
    "    if isinstance(value, str) and re.match(pattern, value):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove website link values from the stage column\n",
    "final_dataset['stage'] = final_dataset['stage'].apply(remove_website_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exchange rates \n",
    "\n",
    "[Source: OFX](https://www.ofx.com/en-au/forex-news/historical-exchange-rates/yearly-average-rates/)\n",
    "```bash\n",
    "exchange_rates = {\n",
    "    2018: 0.014649,\n",
    "    2019: 0.014209,\n",
    "    2020: 0.013501,\n",
    "    2021: 0.013527\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean amount values\n",
    "def floater(string):\n",
    "    try:\n",
    "        string = float(string)\n",
    "    except ValueError:\n",
    "        string = np.nan\n",
    "    \n",
    "    return string\n",
    "    \n",
    "def clean_amount(row): \n",
    "    amount = row[0]    \n",
    "    year   = row['year'] \n",
    "    \n",
    "    # Source: https://www.ofx.com/en-au/forex-news/historical-exchange-rates/yearly-average-rates/\n",
    "    exchange_rates = {\n",
    "        2018: 0.014649,\n",
    "        2019: 0.014209,\n",
    "        2020: 0.013501,\n",
    "        2021: 0.013527\n",
    "    }\n",
    "    \n",
    "    exchange_rate = exchange_rates[year]   \n",
    "    \n",
    "    # Convert to string\n",
    "    amount = str(amount)   \n",
    "    \n",
    "    if isinstance(amount, str):        \n",
    "        # Set of elements to replace\n",
    "        to_replace = {' ', ','}\n",
    "\n",
    "        # Replace each element in the set with an empty string\n",
    "        for r in to_replace:\n",
    "            amount = amount.replace(r, '')        \n",
    "                        \n",
    "        if amount == '' or amount == 'â€”': \n",
    "            amount = np.nan\n",
    "        # If the amount is in INR (Indian Rupees), convert it to USD using the conversion rate of the year\n",
    "        elif 'â‚¹' in amount:\n",
    "            amount = amount.replace('â‚¹', '')\n",
    "            amount = floater(amount) * exchange_rate\n",
    "        \n",
    "        # If the amount is in USD, remove the '$' symbol and convert it to a float\n",
    "        elif '$' in amount:\n",
    "            amount = amount.replace('$', '')\n",
    "            amount = floater(amount)\n",
    "        else:\n",
    "            amount = floater(amount)\n",
    "\n",
    "    \n",
    "    return amount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If the investor value is a number or contains $, the correct value for amount if missing is the investor value, the correct value for stage is the old amount value and the investor value becomes NaN or missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify rows where 'investor' value is numeric using clean amount function\n",
    "mask = final_dataset[['investor', 'year']].apply(lambda row: pd.notna(clean_amount(row)), axis=1)\n",
    "\n",
    "# Update 'stage' column with the 'amount' value if stage is NaN\n",
    "stage_mask = final_dataset['stage'].isna()\n",
    "final_dataset.loc[mask & stage_mask, 'stage']    = final_dataset.loc[mask, 'amount']\n",
    "\n",
    "# Update 'amount' column with 'investor' value\n",
    "final_dataset.loc[mask, 'amount']                = final_dataset.loc[mask, 'investor']\n",
    "\n",
    "# Set 'investor' to NaN\n",
    "final_dataset.loc[mask, 'investor']              = np.nan\n",
    "         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If the 'stage' value is a number or contains $, the column 'what_it does' becomes its old value concatenated with the value in the 'founder' column. The correct value for 'founder' is the 'investor' value and the correct value for 'investor' is the 'amount' value and correct 'amount' becomes the old 'stage' value while the correct value for 'stage' is NaN or missing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify rows where 'stage' value is numeric using clean amount function\n",
    "mask = final_dataset[['stage', 'year']].apply(lambda row: pd.notna(clean_amount(row)), axis=1)\n",
    "\n",
    "# Update the 'what_it_does' column to its concatenation with 'founder' value\n",
    "old_what_it_does = final_dataset.loc[mask, 'what_it_does']\n",
    "old_founder      = final_dataset.loc[mask, 'founders']\n",
    "\n",
    "final_dataset.loc[mask, 'what_it_does']    = old_what_it_does.fillna('') + ' ' + old_founder.fillna('')\n",
    "\n",
    "# Update 'founder' column using the old 'investor' value\n",
    "final_dataset.loc[mask, 'founders']        = final_dataset.loc[mask, 'investor']\n",
    "\n",
    "# Update 'investor' column using the old 'amount' value\n",
    "final_dataset.loc[mask, 'investor']        = final_dataset.loc[mask, 'amount']\n",
    "\n",
    "# Update 'amount' column using the old 'stage' value\n",
    "final_dataset.loc[mask, 'amount']          = final_dataset.loc[mask, 'stage']\n",
    "\n",
    "# Set 'stage' to NaN\n",
    "final_dataset.loc[mask, 'stage']           = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean and convert amounts to USD and rename colume from amount to amount($)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and convert amounts to USD considering the average exchange rate per year\n",
    "final_dataset['amount'] = final_dataset[['amount','year']].apply(lambda row: clean_amount(row), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset.rename(columns={'amount': 'amount($)'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the Headquarter Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix headquarter values, take every word before the first comma as headqurter\n",
    "def splitter(value):\n",
    "    \"\"\"\n",
    "    Splits a string by comma and returns the first part.\n",
    "\n",
    "    Args:\n",
    "        value (str or None): The value to be split.\n",
    "\n",
    "    Returns:\n",
    "        str or None: The first part of the string before the first comma, or the original value if the input is not a string.\n",
    "    \"\"\"\n",
    "    return value.split(',')[0] if isinstance(value, str) else value\n",
    "\n",
    "final_dataset['headquarter'] = [splitter(hq) for hq in final_dataset['headquarter']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Sector Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heal_column_from_chars(df: pd.DataFrame, column: str = 'sector', chars: list = [',', ' ', '&', 'and', '/']) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a dataframe with the sector column having the least redundant value for sector.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to clean.\n",
    "        column (str): The name of the column to clean. Defaults to 'sector'.\n",
    "        chars (list): A list of characters or substrings to handle. Defaults to [',', ' ', '&', 'and', '/']. Always start with ',' for sector \n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with the specified column cleaned.\n",
    "    \"\"\"\n",
    "    \n",
    "    def count_char(value, char):\n",
    "        \"\"\"\n",
    "        Finds all occurrences of char in value and returns the count.\n",
    "        \n",
    "        Parameters:\n",
    "            value (str): The string in which to search for occurrences of char.\n",
    "            char (str): The character to count occurrences of.\n",
    "        \n",
    "        Returns:\n",
    "            int: The count of occurrences of char in value.\n",
    "        \"\"\"\n",
    "        # Use re.findall to find all occurrences of char in value and return the count\n",
    "        return len(re.findall(re.escape(char), str(value)))\n",
    "    \n",
    "    def char_to_nochar_value(char_value, char, no_char_column):\n",
    "        \"\"\"\n",
    "        Find the equivalent value of char_value in no_char_column.\n",
    "        \n",
    "        Parameters:\n",
    "            char_value (str): The string to process.\n",
    "            char (str): The character to split the char_value string.\n",
    "            no_char_column (pd.Series): The column containing unique values to search for the equivalent value.\n",
    "        \n",
    "        Returns:\n",
    "            str: The equivalent value found in no_char_column.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Default no char value        \n",
    "        no_char_value = char_value \n",
    "        \n",
    "        # Split by char, if char='and', \"Food and Beverage\" = [\"Food \", \" Beverage\"]\n",
    "        char_value    = char_value.split(char) \n",
    "        \n",
    "        # If char is comma ',' or if char_value is not amongst no_char_column\n",
    "        no_char_value = char_value[0] if char == ',' else no_char_value\n",
    "        \n",
    "        other_value = ''\n",
    "            \n",
    "        def find_index(where, value):\n",
    "            \"\"\"\n",
    "            Find the index where the given value matches the elements in the Series 'where'.\n",
    "            \n",
    "            Parameters:\n",
    "                where (pd.Series): The Series containing strings to search for the value.\n",
    "                value (str): The value to search for.\n",
    "            \n",
    "            Returns:\n",
    "                np.ndarray: The array of indices where the value matches in the Series.\n",
    "            \"\"\"\n",
    "            return np.where(where.str.lower().unique() == value.strip().lower())[0]\n",
    "            \n",
    "        for value in char_value: \n",
    "            other_value = other_value + ' ' + value\n",
    "            # Find the index of value in unique no_char_column\n",
    "            indexof_value = find_index(where=no_char_column, value=value)\n",
    "            \n",
    "            # Find the index of other value in unique no_char_column\n",
    "            indexof_othervalue = find_index(where=no_char_column, value=value)\n",
    "            \n",
    "            if indexof_value.size == 1:\n",
    "                # If a unique match is found, update no_char_value\n",
    "                no_char_value = no_char_column.unique()[indexof_value[0]]              \n",
    "                break\n",
    "            elif indexof_othervalue.size == 1:  \n",
    "                # If a unique match is found, update no_char_value\n",
    "                no_char_value = no_char_column.unique()[indexof_othervalue[0]]              \n",
    "                break\n",
    "            else:\n",
    "                pass\n",
    "                \n",
    "        return no_char_value\n",
    "        \n",
    "    for char in chars: \n",
    "                \n",
    "        char_list = [count_char(value, char) for value in df[column]]\n",
    "                \n",
    "        char_threshold =  0\n",
    "\n",
    "        mask_char      = [x > char_threshold for x in char_list]    # Mask Boolean\n",
    "\n",
    "        mask_no_char   = [not x for x in mask_char]\n",
    "        \n",
    "        char_column    = df.loc[mask_char, column]\n",
    "\n",
    "        no_char_column = df.loc[mask_no_char, column]                \n",
    "\n",
    "        # Convert 'char' column to no 'char' column in the dataframe if there is a no 'char' equivalent    \n",
    "        df.loc[mask_char, column] = char_column.apply(lambda x: char_to_nochar_value(x, char, no_char_column))\n",
    "    \n",
    "    return df  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the sector column\n",
    "final_dataset = heal_column_from_chars(final_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sector- handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_sector(value):\n",
    "    \"\"\"\n",
    "    Fill missing sector values based on the content of 'what_it_does' column.\n",
    "\n",
    "    Parameters:\n",
    "    value (str): The content of the 'what_it_does' column.\n",
    "\n",
    "    Returns:\n",
    "    str: The assigned sector based on the content of 'what_it_does'.\n",
    "    \"\"\"\n",
    "    stopwords   = ['it', \"a\", \"an\", \"the\", \"and\", \"but\", \"or\"]  # Add other stopwords to exclude\n",
    "    \n",
    "    # Add more sectors and keywords\n",
    "    sector_keywords = {\n",
    "        'Technology': ['platform', 'platforms', 'platform.', 'applications', 'digital', 'digitizes'],\n",
    "        'Waste management': ['waste'],\n",
    "        'Skill development': ['skill, development'], \n",
    "        'Commerce': ['ecommerce'],\n",
    "        'Cosmetics':['skincare'],\n",
    "        'Rental': ['space'],\n",
    "        'HR': ['workforce'],\n",
    "        'Finance': ['financial'],\n",
    "        'Automobile': ['tyre'],\n",
    "        'EdTech': ['edutech']\n",
    "                        \n",
    "    }\n",
    "        \n",
    "    sector  = 'Others' # Default sector if no match is found\n",
    "    \n",
    "    sectors = final_dataset['sector']\n",
    "    \n",
    "    values  = [v for v in value.split(' ') if v.lower() not in stopwords]\n",
    "    \n",
    "    for v in values:\n",
    "         # Find the index of the sector in the sectors column\n",
    "        index_sector = np.where(sectors.str.lower().unique() == v.lower())[0] # Find the index of sector in unique sectors\n",
    "        \n",
    "        # If a match is found, assign the corresponding sector\n",
    "        if index_sector.size == 1:        \n",
    "            sector = sectors.unique()[index_sector[0]]              \n",
    "            break \n",
    "        # If no match is found, search for the word, v in the sector keywords dictionary and assign the corresponding sector \n",
    "        else:\n",
    "            sector = next((sector for sector, keywords in sector_keywords.items() if v.lower() in keywords), sector)\n",
    "    \n",
    "    return sector\n",
    "\n",
    "mask = final_dataset['sector'].isna()\n",
    "\n",
    "final_dataset.loc[mask, 'sector'] = final_dataset.loc[mask, 'what_it_does'].apply(fill_missing_sector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace falsely unique values or actual duplicates in categorical and string columns with their first occurence in the final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heuristic Function to replace actual duplicates with first occurrence\n",
    "def replace_col_duplicates(column):\n",
    "    \"\"\"\n",
    "        Replaces duplicates values (identified through some heuristics) in a column with their first occurrence \n",
    "        Although the first occurrence might not be the best representation but it allows for consistency in values\n",
    "        \n",
    "        Parameter: column\n",
    "        Returns  : column with consistent representation of values\n",
    "    \n",
    "    \"\"\"\n",
    "    actual_strings = {}  # Dictionary to store the first occurrence of each modified string\n",
    "    \n",
    "    def replace_string(string):\n",
    "        actual_string = re.sub(r'[^\\w]', '', string).lower() if isinstance(string, str) else string   # Replace all special characters including whitespaces with '' \n",
    "        if actual_string in actual_strings:\n",
    "            return actual_strings[actual_string]\n",
    "        else:\n",
    "            actual_strings[actual_string] = string\n",
    "            return string\n",
    "    return column.apply(replace_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_of_interest = ['company_brand', 'headquarter', 'sector', 'founders', 'investor', 'stage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_values(columns):\n",
    "    for col in columns:\n",
    "        # Print count of unique items in columns of interest\n",
    "        print(f'{col}: {len(final_dataset[col].unique())}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of unique values before running replace_col_duplicates function\n",
    "count_unique_values(columns_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the replace actual duplicates function to the string or categorical columns\n",
    "for col in columns_of_interest:\n",
    "    final_dataset[col] = replace_col_duplicates(final_dataset[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of unique values after running replace_col_duplicates function\n",
    "count_unique_values(columns_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of missing values in the columns of final dataset\n",
    "final_dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop duplicated rows in final data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicated from final dataset\n",
    "final_dataset.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, check the number of missing values in the columns of final dataset\n",
    "final_dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change each column (founded and year) to appropriate Data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Founded is year and datatype should be integer 32 handling missing values gracefuly\n",
    "final_dataset['founded'] = final_dataset['founded'].astype('Int32') # Int32 instead of int32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year does not need to be int64 but int32\n",
    "final_dataset['year'] = final_dataset['year'].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle missing values in the columns of the final dataset except company_brand, what_it_does and year which have no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Founded column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset['founded'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values in 'founded' column with median by year\n",
    "founded_median_by_year = final_dataset['founded']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Headquarter column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sector column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Founders column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investor column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amount column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset['stage'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill missing values and replace Undisclosed values in 'stage' column with 'Venture - Series Unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in 'stage' column with 'Venture - Series Unknown\n",
    "final_dataset['stage'].fillna('Venture - Series Unknown', inplace=True)\n",
    "\n",
    "# Replace Undisclosed values in 'stage' column with 'Venture - Series Unknown\n",
    "final_dataset['stage'].replace(to_replace='Undisclosed', value='Venture - Series Unknown', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset['stage'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values in the Founders Column\n",
    "### The dataset had 545 missing values for the founders, so we decided to drop the column for founders as we will not need it for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset.drop('founders', axis=1, inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values in the Sector column\n",
    "### The sector column had 18 missing values so we filled them with the value Unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset['sector'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling the missing values in sector column with Unkownn\n",
    "final_dataset['sector'].fillna(\"Unknown\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(re.sub(r'[^\\,]', '', \"ar345547686,,4,5,,\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing values in the Investor Column\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mode for each sector in the 'investor' column\n",
    "mode_per_sector = final_dataset.groupby('sector')['investor'].transform(lambda x: x.mode().iloc[0] if not x.mode().empty else \"Unknown\")\n",
    "\n",
    "\n",
    "# Fill missing values in the 'investor' column with the calculated mode per sector\n",
    "final_dataset['investor'].fillna(mode_per_sector, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling null values in the column for Head Quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "np.random.seed(2024)\n",
    "\n",
    "# Identify the rows with missing 'headquarter' values\n",
    "mask = final_dataset['headquarter'].isna()\n",
    "\n",
    "# Get non-missing values for final_dataset['headquarter']\n",
    "non_missing_hq = final_dataset.loc[mask==False, 'headquarter'] \n",
    "\n",
    "hq_missing     = mask.sum()\n",
    "\n",
    "# Randomly sample non-missing values to fill missing values of size hq_missing, 114\n",
    "hq_random      = np.random.choice(non_missing_hq, size=hq_missing)\n",
    "\n",
    "# Fill missing values with randomly sampled headquarter values\n",
    "final_dataset.loc[mask, 'headquarter'] = hq_random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling Null Values in Founded Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the median founded grouped by year values\n",
    "founded_median_per_year = final_dataset.groupby('year')['founded'].median()\n",
    "\n",
    "# Calculate the overall median of median_per_year\n",
    "overall_median_per_year = founded_median_per_year.median()\n",
    "\n",
    "# Fill NaN values in founded_median_per_year with the overall median of median_per_year\n",
    "founded_median_per_year.fillna(overall_median_per_year, inplace=True)\n",
    "\n",
    "# Fill missing values in the 'founded' column with the calculated median per year\n",
    "final_dataset['founded'].fillna(final_dataset['year'].map(founded_median_per_year), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling Null Values in the Amount column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_median = final_dataset['amount($)'].median()\n",
    "final_dataset['amount($)'].fillna(amount_median, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save datasets as flat files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset from Microsoft SQL database\n",
    "# first_dataset.to_csv('DataSets/individual_csv/startup_funding2020-2021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final merged dataset with cleaned column names and cleaned amount values\n",
    "# final_dataset.to_csv('DataSets/final_csv/startup_funding2018-2021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startups_in_bangalore = final_dataset[final_dataset['headquarter']=='Bangalore']\n",
    "startups_not_in_bangalore = final_dataset[final_dataset['headquarter']!='Bangalore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t,p =stats.ttest_ind(startups_in_bangalore['amount($)'],startups_not_in_bangalore['amount($)'])\n",
    "print(f't-value:{t}')\n",
    "print(f'p-value:{p}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Significance level\n",
    "alpha = 0.05\n",
    "\n",
    "#Compare p-value to the significance level\n",
    "if p < alpha:\n",
    "    print(\"We reject the null hypothesis. Which is there is no siginificate difference in the amount of funding between startups in Bangalore.\")\n",
    "else:\n",
    "   print(\"We failed to reject the null hypothesis. There is a siginificate difference in the amount of funding between startups in Bangalore.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_dataset['headquarter'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset['stage'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What sectors have shown the highest growth in terms of funding received over the past four years?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_sector_2018 = final_dataset[final_dataset['year']==2018]\n",
    "year_sector_2019 = final_dataset[final_dataset['year']==2019]\n",
    "year_sector_2020 = final_dataset[final_dataset['year']==2020]\n",
    "year_sector_2021 = final_dataset[final_dataset['year']==2021]\n",
    "\n",
    "\n",
    "#Group 2018 data by sector and get the sum of the amount\n",
    "sector_year_funding_2018 = year_sector_2018.groupby('sector')['amount($)'].sum()/ 1e9\n",
    "sector_year_funding_2018_reset = sector_year_funding_2018.reset_index().sort_values(by='amount($)', ascending=False).head(5)\n",
    "\n",
    "#Group 2019 data by sector and get the sum of the amount\n",
    "sector_year_funding_2019 = year_sector_2019.groupby('sector')['amount($)'].sum()/ 1e9\n",
    "sector_year_funding_2019_reset = sector_year_funding_2019.reset_index().sort_values(by='amount($)', ascending=False).head(5)\n",
    "\n",
    "#Group 2020 data by sector and get the sum of the amount\n",
    "sector_year_funding_2020 = year_sector_2020.groupby('sector')['amount($)'].sum()/ 1e9\n",
    "sector_year_funding_2020_reset = sector_year_funding_2020.reset_index().sort_values(by='amount($)', ascending=False).head(5)\n",
    "\n",
    "#Group 2021 data by sector and get the sum of the amount\n",
    "sector_year_funding_2021 = year_sector_2021.groupby('sector')['amount($)'].sum()/ 1e9\n",
    "sector_year_funding_2021_reset = sector_year_funding_2021.reset_index().sort_values(by='amount($)', ascending=False).head(5)\n",
    "\n",
    "sector_year_funding_2018_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(16, 9))\n",
    "\n",
    "# Function to annotate each bar with the amount\n",
    "def annotate_bars(data, ax):\n",
    "    for index, value in enumerate(data['amount($)']):\n",
    "        ax.text(value, index, f'{value:.2f}B', ha='left', va='center', fontsize=10)\n",
    "\n",
    "# Plotting for 2018\n",
    "sns.barplot(x='amount($)', y='sector', data=sector_year_funding_2018_reset, palette=\"viridis\", ax=axes[0, 0])\n",
    "annotate_bars(sector_year_funding_2018_reset, axes[0, 0])\n",
    "axes[0, 0].set_title('Top 5 Sectors with Highest Growth in Funding in 2018')\n",
    "axes[0, 0].set_xlabel('Average Growth in Funding (In Billions)')\n",
    "axes[0, 0].set_ylabel('Top 5 Sectors in 2018')\n",
    "\n",
    "# Plotting for 2019\n",
    "sns.barplot(x='amount($)', y='sector', data=sector_year_funding_2019_reset, palette=\"viridis\", ax=axes[0, 1])\n",
    "annotate_bars(sector_year_funding_2019_reset, axes[0, 1])\n",
    "axes[0, 1].set_title('Top 5 Sectors with Highest Growth in Funding in 2019')\n",
    "axes[0, 1].set_xlabel('Average Growth in Funding (In Billions)')\n",
    "axes[0, 1].set_ylabel('Top 5 Sectors in 2019')\n",
    "\n",
    "# Plotting for 2020\n",
    "sns.barplot(x='amount($)', y='sector', data=sector_year_funding_2020_reset, palette=\"viridis\", ax=axes[1, 0])\n",
    "annotate_bars(sector_year_funding_2020_reset, axes[1, 0])\n",
    "axes[1, 0].set_title('Top 5 Sectors with Highest Growth in Funding in 2020')\n",
    "axes[1, 0].set_xlabel('Average Growth in Funding (In Billions)')\n",
    "axes[1, 0].set_ylabel('Top 5 Sectors in 2020')\n",
    "\n",
    "# Plotting for 2021\n",
    "sns.barplot(x='amount($)', y='sector', data=sector_year_funding_2021_reset, palette=\"viridis\", ax=axes[1, 1])\n",
    "annotate_bars(sector_year_funding_2021_reset, axes[1, 1])\n",
    "axes[1, 1].set_title('Top 5 Sectors with Highest Growth in Funding in 2021')\n",
    "axes[1, 1].set_xlabel('Average Growth in Funding (In Billions)')\n",
    "axes[1, 1].set_ylabel('Top 5 Sectors in 2021')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Convert the 'growth' values to billions\n",
    "# sector_year_funding_2018_reset['growth_billions'] = sector_year_funding_2018_reset['amount($)'] / 1e9\n",
    "\n",
    "# # Plotting using seaborn\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# bar_plot = sns.barplot(x='growth_billions', y='sector', data=sector_year_funding_2018_reset, palette=\"viridis\")\n",
    "\n",
    "# plt.title('Top 5 Sectors with Highest Growth in Funding in 2018')\n",
    "# plt.xlabel('Average Growth in Funding (In Billions)')\n",
    "# plt.ylabel('Top 5 Sectors in 2018')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Convert the 'growth' values to billions\n",
    "# sector_year_funding_2019_reset['growth_billions'] = sector_year_funding_2019_reset['amount($)'] / 1e9\n",
    "\n",
    "# # Plotting using seaborn\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# bar_plot = sns.barplot(x='growth_billions', y='sector', data=sector_year_funding_2019_reset, palette=\"viridis\")\n",
    "\n",
    "\n",
    "# plt.title('Top 5 Sectors with Highest Growth in Funding in 2019')\n",
    "# plt.xlabel('Average Growth in Funding (In Billions)')\n",
    "# plt.ylabel('Top 5 Sectors in 2019')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Convert the 'growth' values to billions\n",
    "# sector_year_funding_2020_reset['growth_billions'] = sector_year_funding_2020_reset['amount($)'] / 1e9\n",
    "\n",
    "# # Plotting using seaborn\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# bar_plot = sns.barplot(x='growth_billions', y='sector', data=sector_year_funding_2020_reset, palette=\"viridis\")\n",
    "\n",
    "\n",
    "# plt.title('Top 5 Sectors with Highest Growth in Funding in 2020')\n",
    "# plt.xlabel('Average Growth in Funding (In Billions)')\n",
    "# plt.ylabel('Top 5 Sectors in 2020')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Convert the 'growth' values to billions\n",
    "# sector_year_funding_2021_reset['growth_billions'] = sector_year_funding_2021_reset['amount($)'] / 1e9\n",
    "\n",
    "# # Plotting using seaborn\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# bar_plot = sns.barplot(x='growth_billions', y='sector', data=sector_year_funding_2021_reset, palette=\"viridis\")\n",
    "\n",
    "\n",
    "# plt.title('Top 5 Sectors with Highest Growth in Funding in 2021')\n",
    "# plt.xlabel('Average Growth in Funding (In Billions)')\n",
    "# plt.ylabel('Top 5 Sectors in 2021')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What locations within India have emerged as the primary hubs for startup activity and investment, and what factors contribute to their prominence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Grouping the dataset based on the headquarters\n",
    "# startups_by_location = final_dataset.groupby('headquarter')['amount($)'].sum()\n",
    "\n",
    "# #Reset index and get the top 10 locations\n",
    "# top_10_locations = startups_by_location.reset_index().sort_values(by ='amount($)', ascending = False).head(10)\n",
    "\n",
    "# # top_10_locations\n",
    "\n",
    "# # #Convert the 'growth' values to billions\n",
    "# top_10_locations['growth_billions'] = top_10_locations['amount($)'] / 1e9\n",
    "\n",
    "# # Plotting using seaborn\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# bar_plot = sns.barplot(x='growth_billions', y='headquarter', data=top_10_locations, palette=\"viridis\")\n",
    "# # Add the numbers near the bars\n",
    "# for i, v in enumerate(top_10_locations['growth_billions']):\n",
    "#     bar_plot.text(v + 3, i + .25, str(v), color='black', fontweight='light')\n",
    "\n",
    "# plt.title('Top 10 locations with the Highest Startup activity and Investment')\n",
    "# plt.xlabel('Average Investment in Funding (In Billions)')\n",
    "# plt.ylabel('Top 10 locations')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the dataset based on the headquarters\n",
    "startups_by_location = final_dataset.groupby('headquarter')['amount($)'].sum()\n",
    "# Reset the index before assigning column names\n",
    "top_10_locations_by_investment = startups_by_location.reset_index().sort_values(by ='amount($)', ascending = False).head(10)\n",
    "top_10_locations_by_investment['growth_billions'] = top_10_locations_by_investment['amount($)'] / 1e9\n",
    "\n",
    "# Assign column names\n",
    "top_10_locations_by_investment.columns = ['headquarter', 'amount($)', 'growth_billions']\n",
    "# top_10_locations_by_investment\n",
    "\n",
    "# Initialize the Nominatim geocoder\n",
    "geolocator = Nominatim(user_agent=\"my_geocoder\")\n",
    "\n",
    "# Function to retrieve coordinates for a location\n",
    "def get_coordinates(location):\n",
    "    try:\n",
    "        location_info = geolocator.geocode(location)\n",
    "        if location_info:\n",
    "            return location_info.latitude, location_info.longitude\n",
    "        else:\n",
    "            print(f\"Warning: Coordinates not found for {location}. Skipping.\")\n",
    "            return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving coordinates for {location}: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# Apply the function to get coordinates for each location\n",
    "top_10_locations_by_investment[['Latitude', 'Longitude']] = top_10_locations_by_investment['headquarter'].apply(lambda x: pd.Series(get_coordinates(x)))\n",
    "top_10_locations_by_investment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a map centered around the first location\n",
    "map_top_10_locations_by_investment= folium.Map(location=[top_10_locations_by_investment['Latitude'].iloc[0], top_10_locations_by_investment['Longitude'].iloc[0]], zoom_start=3)\n",
    "marker_cluster = MarkerCluster().add_to(map_top_10_locations_by_investment)\n",
    "\n",
    "# Add a marker for each location to the MarkerCluster\n",
    "for idx, row in top_10_locations_by_investment.iterrows():\n",
    "    radius = int(row['growth_billions'] / 1e9)\n",
    "    folium.Marker(\n",
    "        location=[row['Latitude'], row['Longitude']],\n",
    "        popup=folium.Popup(('<strong><font color =\"green\">'+row['headquarter']+'</font></strong><br>'+\n",
    "                            '<strong>Total Investment (Billions): </strong><font color =\"blue\">'+str(row['growth_billions'])+'</font><br>'), max_width=250),\n",
    "    ).add_to(marker_cluster)\n",
    "map_top_10_locations_by_investment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# startups_by_location2 = final_dataset['headquarter'].value_counts()\n",
    "\n",
    "# # Reset index and get the top 10 locations\n",
    "# top_10_locations2 = startups_by_location2.head(10).sort_values(ascending=False).reset_index()\n",
    "# top_10_locations2.columns = ['headquarter', 'count']\n",
    "\n",
    "\n",
    "# # Plotting using seaborn\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# ax = sns.barplot(x='count', y='headquarter', data=top_10_locations2, palette=\"viridis\")\n",
    "\n",
    "# # Add the numbers near the bars\n",
    "# for i, v in enumerate(top_10_locations2['count']):\n",
    "#     ax.text(v + 3, i + .25, str(v), color='black', fontweight='light')\n",
    "\n",
    "# plt.title('Top 10 locations with the Highest Startups')\n",
    "# plt.xlabel('Number of Startups')\n",
    "# plt.ylabel('Top 10 locations')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startups_by_location2 = final_dataset['headquarter'].value_counts()\n",
    "top_10_locations = startups_by_location2.head(10).sort_values(ascending=False).reset_index()\n",
    "top_10_locations.columns = ['Location', 'Number of Startups']\n",
    "\n",
    "# Initialize the Nominatim geocoder\n",
    "geolocator = Nominatim(user_agent=\"my_geocoder\")\n",
    "\n",
    "# Function to retrieve coordinates for a location\n",
    "def get_coordinates(location):\n",
    "    try:\n",
    "        location_info = geolocator.geocode(location)\n",
    "        if location_info:\n",
    "            return location_info.latitude, location_info.longitude\n",
    "        else:\n",
    "            print(f\"Error retrieving coordinates for {location}: Location not found\")\n",
    "            return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving coordinates for {location}: {str(e)}\")\n",
    "        return None, None\n",
    "# Apply the function to get coordinates for each location\n",
    "top_10_locations[['Latitude', 'Longitude']] = top_10_locations['Location'].apply(lambda x: pd.Series(get_coordinates(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map centered around the first location\n",
    "map_top_10_locations = folium.Map(location=[top_10_locations['Latitude'].iloc[0], top_10_locations['Longitude'].iloc[0]], zoom_start=3)\n",
    "\n",
    "# Create a MarkerCluster to cluster the markers\n",
    "marker_cluster = MarkerCluster().add_to(map_top_10_locations)\n",
    "\n",
    "# Add a marker for each location to the MarkerCluster\n",
    "for idx, row in top_10_locations.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['Latitude'], row['Longitude']],\n",
    "        popup=folium.Popup(('<strong><font color =\"green\">'+row['Location']+'</font></strong><br>'+\n",
    "                            '<strong>Number of Startups: </strong><font color =\"blue\">'+str(row['Number of Startups'])+'</font><br>'), max_width=250),\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "# Display the map\n",
    "map_top_10_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Grouping the dataset based on the headquarters\n",
    "# startups_by_location2 = final_dataset['headquarter'].value_counts()\n",
    "\n",
    "# #Reset index and get the top 10 locations\n",
    "# top_10_locations2 = startups_by_location2.head(10).sort_values(ascending = False).reset_index()\n",
    "\n",
    "\n",
    "# # Plotting using seaborn\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# ax = sns.barplot(x='count', y='headquarter', data=top_10_locations2, palette=\"viridis\")\n",
    "\n",
    "# plt.title('Top 10 locations with the Highest Startups')\n",
    "# plt.xlabel('Number of Startups')\n",
    "# plt.ylabel('Top 10 locations')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are there any notable differences in funding patterns between early-stage startups and more established companies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the median year for the founded coulmn\n",
    "overall_median_per_year  # The overall median year is 2016\n",
    "\n",
    "final_dataset['stage_of_startup'] = np.where(final_dataset['founded'] >= overall_median_per_year, 'Early Stage', 'Established')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the count of each category in the 'stage' column\n",
    "stage_distribution = final_dataset['stage_of_startup'].value_counts()\n",
    "\n",
    "# Print or visualize the distribution\n",
    "print(stage_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure and axis\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot countplot for 'stage'\n",
    "sns.countplot(x='stage_of_startup', data=final_dataset, ax=ax, palette=\"viridis\")\n",
    "\n",
    "# Display the count above each bar\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "plt.title('Distribution of Companies by Stage')\n",
    "plt.xlabel('Stage')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stage_stats = final_dataset[final_dataset['stage_of_startup'] == 'Early Stage']['amount($)'].describe()\n",
    "established_stats = final_dataset[final_dataset['stage_of_startup'] == 'Established']['amount($)'].describe()\n",
    "\n",
    "print(\"Early Stage Funding Statistics:\")\n",
    "print(early_stage_stats)\n",
    "\n",
    "print(\"\\nEstablished Funding Statistics:\")\n",
    "print(established_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####\n",
    "**Mean Funding Amount:** Early-stage companies have a slightly lower mean funding amount ($83.88 million) compared to established companies ($136.97 million).\n",
    "\n",
    "**Variability (Standard Deviation):** Both groups exhibit high variability in funding amounts, as indicated by the large standard deviations.\n",
    "\n",
    "**Minimum and Maximum Funding:** Both groups have a wide range of funding amounts, with early-stage companies having a minimum of $720 and a maximum of $150 billion, while established companies range from $40,900 to $70 billion.\n",
    "\n",
    "**Percentiles (Q1, Median, Q3):** Early-stage companies generally have lower funding amounts at each percentile compared to established companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stage_startups = final_dataset.groupby('stage_of_startup')['amount($)'].sum().reset_index()\n",
    "early_stage_startups['amount_in_billions'] = early_stage_startups['amount($)'] / 1e9\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(x='stage_of_startup', y='amount_in_billions', data=early_stage_startups, ci=None, palette=\"viridis\")\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height():.2f}B', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "plt.title('Comparison of Funding Amounts between Early Stage and Established Companies')\n",
    "plt.xlabel('Stage')\n",
    "plt.ylabel('Amount ($B)')\n",
    "plt.show()\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis Testing\n",
    "\n",
    "#### \n",
    "Null Hypothesis(H0): There is no significant difference in the average funding amounts between early-stage startups and established companies.\n",
    "\n",
    "Alternative Hypothesis(H1): There is a significant difference in the average funding amounts between early-stage startups and established companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stage_funding = final_dataset[final_dataset['stage_of_startup'] == 'Early Stage']['amount($)']\n",
    "established_funding = final_dataset[final_dataset['stage_of_startup'] == 'Established']['amount($)']\n",
    "\n",
    "t_stat, p_value = ttest_ind(early_stage_funding, established_funding, nan_policy='omit')\n",
    "\n",
    "\n",
    "# Set Significance level\n",
    "alpha = 0.05\n",
    " \n",
    "#Compare p-value to the significance level\n",
    "if p_value < alpha:\n",
    "    print('We reject the Null hypothesis.There is no significant difference in the average funding amounts between early_stage startups and established companies')\n",
    "else:\n",
    "   print(\"We fail to reject the Null hypothesis\")\n",
    "\n",
    "\n",
    "# print(f\"T-statistic: {t_stat}\")\n",
    "# print(f\"P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which sectors recieve the lowest level of funding and which sectors recieve the highest levels of funding in India"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by sector and get the sum of the amount\n",
    "sector_year_funding = final_dataset.groupby('sector')['amount($)'].sum()\n",
    "sector_year_funding_reset = sector_year_funding.reset_index().sort_values(by='amount($)', ascending=False).head(5)\n",
    "\n",
    "# Convert the 'growth' values to billions\n",
    "sector_year_funding_reset['growth_billions'] = sector_year_funding_reset['amount($)'] / 1e9\n",
    "\n",
    "def annotate_bars(data, ax):\n",
    "    for index, value in enumerate(data['growth_billions']):\n",
    "        ax.text(value, index, f'{value:.2f}B', ha='left', va='center', fontsize=10)\n",
    "\n",
    "# Plotting using seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(x='growth_billions', y='sector', data=sector_year_funding_reset, palette=\"viridis\")\n",
    "\n",
    "plt.title('Top 5 Sectors with Highest Growth in Funding')\n",
    "plt.xlabel('Average Growth in Funding (In Billions)')\n",
    "plt.ylabel('Top 5 Sectors')\n",
    "\n",
    "# Annotate the bars with values\n",
    "annotate_bars(sector_year_funding_reset, ax)\n",
    "plt.show()\n",
    "sector_year_funding_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group data by sector and get the sum of the amount\n",
    "sector_year_funding = final_dataset.groupby('sector')['amount($)'].sum()\n",
    "sector_year_funding_reset = sector_year_funding.reset_index().sort_values(by='amount($)', ascending=False).tail(5)\n",
    "\n",
    "#Convert the 'growth' values to billions\n",
    "# sector_year_funding_reset['growth_billions'] = sector_year_funding_reset['amount($)'] / 1e9\n",
    "\n",
    "def annotate_bars(data, ax):\n",
    "    for index, value in enumerate(data['amount($)']):\n",
    "        ax.text(value, index, f'{value:.2f}', ha='left', va='center', fontsize=10)\n",
    "\n",
    "# Plotting using seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(x='amount($)', y='sector', data=sector_year_funding_reset, palette=\"viridis\")\n",
    "\n",
    "annotate_bars(sector_year_funding_reset,ax)\n",
    "plt.title('Least 5 Sectors with Lowest Growth in Funding')\n",
    "plt.xlabel('Average Growth in Funding (In Thousands)')\n",
    "plt.ylabel('Least 5 Sectors')\n",
    "plt.show()\n",
    "sector_year_funding_reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which investors have more impact on startups over the years?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by investor and sum the investment amounts\n",
    "investor_impact = final_dataset.groupby('investor')['amount($)'].sum()\n",
    "\n",
    "# Get the top 10 investors with the highest total investment amounts\n",
    "top_10_investors = investor_impact.nlargest(10)\n",
    "\n",
    "#Return top 10 investors\n",
    "top_10_investors\n",
    "\n",
    "# Reset index and rename the columns\n",
    "top_10_investors_reset = top_10_investors.reset_index()\n",
    "top_10_investors_reset.columns = ['Investor', 'amount']\n",
    "top_10_investors_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'amount' values to billions\n",
    "top_10_investors_reset['amount_billions'] = top_10_investors_reset['amount'] / 1e9\n",
    "\n",
    "# Sort the DataFrame by 'amount_billions' column in descending order\n",
    "top_10_investors_reset = top_10_investors_reset.sort_values(by='amount_billions', ascending=False)\n",
    "\n",
    "# Plotting using seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "bar_plot = sns.barplot(x='amount_billions', y='Investor', data=top_10_investors_reset, palette=\"viridis\")\n",
    "\n",
    "# Add data labels\n",
    "for index, row in top_10_investors_reset.iterrows():\n",
    "    bar_plot.text(row['amount_billions'], index, f'{row[\"amount_billions\"]:.2f}B', va='center')\n",
    "\n",
    "plt.title('Top 10 Investors Impact on Startups Over the Years')\n",
    "plt.xlabel('Total Investment Amount (Billions $)')\n",
    "plt.ylabel('Investor')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made with ðŸ’– [Team Curium](https://github.com/MumoMutiso/Indian-startup-collab)\n",
    "<span style=\"color: #aaaaaa;\">2024</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
